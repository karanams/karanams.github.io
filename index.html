<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<html>
  <head>
	<meta name=viewport content='width=800'>
    <meta name="generator" content="HTML Tidy for Linux/x86 (vers 11 February 2007), see www.w3.org">
    <style type="text/css">
      /* Color scheme from Sergey Karayev and Jon Barron*/
      a {
      color: #1772d0;
      text-decoration:none;
      }
      a:focus, a:hover {
      color: #f09228;
      text-decoration:none;
      }
      body,td,th {
      font-family: 'Lato', Verdana, Helvetica, sans-serif;
      font-size: 14px
      }
      strong {
      font-family: 'Lato', Verdana, Helvetica, sans-serif;
      font-size: 14px;
      }
      heading {
      font-family: 'Lato', Verdana, Helvetica, sans-serif;
      font-size: 18px;
      }
      papertitle {
      font-family: 'Lato', Verdana, Helvetica, sans-serif;
      font-size: 15px;
      font-weight: 700
      }
      name {
      font-family: 'Lato', Verdana, Helvetica, sans-serif;
      font-size: 32px;
      }
	.fade {
	   transition: opacity .2s ease-in-out;
	   -moz-transition: opacity .2s ease-in-out;
	   -webkit-transition: opacity .2s ease-in-out;
	   }
    </style>

    <title>Srikrishna Karanam</title>
    <meta http-equiv="Content-Type" content="text/html; charset=us-ascii">
    <link href='http://fonts.googleapis.com/css?family=Lato:400,700,400italic,700italic' rel='stylesheet' type='text/css'>
  </head>
  <body>
    <table width="800" border="0" align="center" cellspacing="0" cellpadding="0">
      <tr>
        <td>
          <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
            <tr>
              <td width="67%" valign="middle">
                <p align="center">
                  <name>Srikrishna Karanam</name>
                  </font>
                <p align>I am a Research Scientist in the Vision group at <a href="https://www.siemens.com/global/en/home/company/innovation/corporate-technology.html" target="_blank">Siemens Corporate Technology</a>, where I work on Computer Vision and related problems. I earned my Ph.D. in <a href="https://www.ecse.rpi.edu/" target="_blank">Computer and Systems Engineering</a> at <a href="https://www.rpi.edu/" target="_blank">Rensselaer Polytechnic Institute</a>, where my adviser was <a href="https://www.ecse.rpi.edu/~rjradke" target="_blank">Prof. Rich Radke</a>.
                  <br>
                  <br>
                  For an overview of my Ph.D. work, please see <a href="http://www.northeastern.edu/alert/news-article/student-spotlight-srikrishna-karanam/" target="_blank">this article</a> from 
				<a href="http://www.northeastern.edu/alert/" target="_blank"> ALERT@Northeastern</a>.</p>
                <p align=center>
<a href="mailto:srikrishna@ieee.org">Email</a> &nbsp/&nbsp
<a <!href="karanam-cv.pdf">CV</a> 
                </p>
              </td>
              <td width="33%"><img src="karanam.jpg" height=300></td>
            </tr>
          </table>
          <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
		
		  
	  <tr><td>
            <heading>News</heading>
            <ul>	    
	    <li> We are organizing the Second <a href="https://wvbsd.github.io/2019/" target="_blank">Vision with Biased or Scarce Data </a> workshop at  <a href="http://cvpr2019.thecvf.com/" target="_blank">CVPR 2019</a>.</li>
            </ul>
		</td></tr>	
		  
		  	  <tr><td>
            <heading>Data</heading>
            <ul>
	    <li> <a href="http://www.northeastern.edu/alert/transitioning-technology/alert-datasets/alert-airport-re-identification-dataset/" target="_blank">Airport</a> re-identification dataset is now available! Please request access <a href="http://www.northeastern.edu/alert/transitioning-technology/alert-datasets/alert-dataset-request-form-2/" target="_blank">here</a>.</li>  
	    <li> <a href="https://github.com/MZhengRPI/RPIfield" target="_blank"> RPIfield</a>.</li>    
            </ul>
		</td></tr>
					  
		  
            <tr>
              <td>
                <heading>Research</heading>
                <p>I am interested in computer vision, video processing, machine learning and optimization. I am particularly interested in video analytics problems as they occur in large networks of cameras.
				
              </td>
            </tr>
          </table>

          <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
              <tr>
              <td>
                <heading>Preprints</heading>
              </td>
            </tr>
		  
		  </tr>

              <td width="25%"><img src="papers/2018/pipeline.PNG" alt="teaser" width="180" height="90" style="border-style: none">
              <td width="75%" valign="top">
                <p>
                  <a href="https://arxiv.org/pdf/1811.12297.pdf" target="_blank">
                  <papertitle>Incremental Scene Synthesis</papertitle>
                  </a>
                  <br>
                  <a href="http://aldream.net/" target="_blank">Benjamin Planche</a>, <a href="http://xrong.org/" target="_blank">Xuejian Rong</a>, <a href="http://wuziyan.com/" target="_blank">Ziyan Wu</a>, <strong>Srikrishna Karanam</strong>, <a href="http://www.fim.uni-passau.de/en/distributed-information-systems/" target="_blank">Harald Kosch</a>, <a href="http://media-lab.ccny.cuny.edu/wordpress/YLTCCNYHomepage/home.html" target="_blank">YingLi Tian</a>, <a href="https://www.researchgate.net/profile/Andreas_Hutter" target="_blank">Andreas Hutter</a>, <a href="https://scholar.google.de/citations?user=hFUJkFgAAAAJ&hl=en" target="_blank">Jan Ernst</a>
                  <br>
                  <em>arXiv preprint 1811.12297</em>, 2018 <br>	
				  <p></p>
				  <p> We present a method to incrementally generate complete 2D or 3D scenes. Our framework can register observations from a non-localized agent in a global representation, which can be used to synthesize new views as well as fill in gaps in the representation while observing global consistency.  

				  <p></p>
				  </a></p>
              </td>
            </tr>
		  
		  </tr>

              <td width="25%"><img src="papers/2018/mainidea.png" alt="discAtten" width="180" height="90" style="border-style: none">
              <td width="75%" valign="top">
                <p>
                  <a href="https://arxiv.org/pdf/1811.07484" target="_blank">
                  <papertitle>Reducing Visual Confusion with Discriminative Attention</papertitle>
                  </a>
                  <br>
                  <a href="https://scholar.google.com/citations?user=B6JR50gAAAAJ&hl=en" target="_blank">Lezi Wang</a>, <a href="http://wuziyan.com/" target="_blank">Ziyan Wu</a>, <strong>Srikrishna Karanam</strong>, <a href="https://scholar.google.com/citations?user=GBi3LYkAAAAJ&hl=en" target="_blank">Kuan-Chuan Peng, <a href="http://rajatvikramsingh.github.io/" target="_blank">Rajat Vikram Singh</a>
                  <br>
                  <em>arXiv preprint 1811.07484</em>, 2018 <br>	
				  <p></p>
				  <p> We present a new learning framework that makes discriminative attention a principled part of the learning process, showing it can lead to better model interpretability, as well as improved classification accuracy.
				  <p></p>
				  </a></p>
              </td>
            </tr>

<tr>
              <td width="25%"><img src="papers/2018/title.PNG" alt="rgb2cad" width="180" height="180" style="border-style: none">
              <td width="75%" valign="top">
                <p>
                  <a href="https://arxiv.org/pdf/1811.07249" target="_blank">
                  <papertitle>Matching RGB Images to CAD Models for Object Pose Estimation</papertitle>
                  </a>
                  <br>
                  <a href="https://cs.gmu.edu/~ggeorgak/" target="_blank">Georgios Georgakis</a>, <strong>Srikrishna Karanam</strong>, <a href="http://wuziyan.com/" target="_blank">Ziyan Wu</a>, <a href="https://cs.gmu.edu/~kosecka/" target="_blank">Jana Kosecka</a>
                  <br>
                  <em>arXiv preprint 1811.07249</em>, 2018 <br>	
				  <p></p>
				  <p> We present a new method that matches RGB images to rendered depth images of CAD models for object pose estimation. The method does not require either textured CAD models or 3D pose annotations for RGB images during training. This is achieved through a series of constraints that enforce viewpoint and modality invariance for local features, and learn how to select keypoints consistently across modalities.
				  <p></p>
				  </a></p>
              </td>
		  
 	   
		              </tr>

              <td width="25%"><img src="papers/2017/rpifield.PNG" alt="rpifield" width="180" height="180" style="border-style: none">
              <td width="75%" valign="top">
                <p>
                  <a href="https://arxiv.org/pdf/1808.05499.pdf" target="_blank">
                  <papertitle> Measuring the Temporal Behavior of Real-World Person Re-Identification</papertitle>
                  </a>
                  <br>
                  <a href="http://homepages.rpi.edu/~zhengm3/" target="_blank">Meng Zheng</a>, <strong>Srikrishna Karanam</strong>, <a href="https://www.ecse.rpi.edu/~rjradke/index.htm" target="_blank">Richard J. Radke</a>
                  <br>
                  <em>arXiv preprint 1808.05499</em>, 2018 <br>	
				  <a href="https://github.com/MZhengRPI/RPIfield" target="_blank">code & data</a>
				  <p></p>
				  <p> We present a new methodology to practically evaluate re-id algorithms as they are deployed in real-world systems. We introduce a new dataset, <a href="https://drive.google.com/file/d/1GO1zm7vCAJwXgJtoFyUs367_Knz8Ev0A/view" target="_blank">RPIfield</a>, to carefully illustrate the concept. This subsumes our work described <a href="https://arxiv.org/pdf/1706.00553.pdf" target="_blank">here</a> and <a href="http://openaccess.thecvf.com/content_cvpr_2018_workshops/papers/w36/Zheng_RPIfield_A_New_CVPR_2018_paper.pdf" target="_blank">here</a>.</a>
				  <p></p>
				  </a></p>
              </td>
            </tr>


</tr>
		  
          <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
		  
              <tr>
              <td>
                <heading>Papers</heading>
              </td>
            </tr>
		  
</tr>

              <td width="25%"><img src="papers/2018/teaser.png" alt="teaser" width="180" height="120" style="border-style: none">
              <td width="75%" valign="top">
                <p>
                  <a href="https://arxiv.org/pdf/1811.07487" target="_blank">
                  <papertitle>Re-Identification with Consistent Attentive Siamese Networks</papertitle>
                  </a>
                  <br>
                  <a href="http://homepages.rpi.edu/~zhengm3/" target="_blank">Meng Zheng</a>, <strong>Srikrishna Karanam</strong>, <a href="http://wuziyan.com/" target="_blank">Ziyan Wu</a>, <a href="https://www.ecse.rpi.edu/~rjradke/index.htm" target="_blank">Richard J. Radke</a>
                  <br>
                  <em>IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</em>, 2019 <br>	
				  <p></p>
				  <p> We present the first framework for re-id that provides mechanisms to make attention and attention consistency end-to-end trainable in a Siamese learning architecture, resulting in a technique for robust cross-view matching as well as explaining the reasoning for why the model predicts that the two images belong to the same person.
				  <p></p>
				  </a></p>
              </td>
            </tr>
		  
		  <tr>
              <td width="25%"><img src="papers/2016/teaser-pami.PNG" alt="karanam-gou-arxiv16" width="160" height="180" style="border-style: none">
              <td width="75%" valign="top">
                <p>
                  <a href="https://arxiv.org/pdf/1605.09653.pdf" target="_blank">
                  <papertitle> A Systematic Evaluation and Benchmark for Person Re-Identification: Features, Metrics, and Datasets</papertitle>
                  </a>
                  <br>
                  <strong>Srikrishna Karanam</strong>*, <a href="https://neu-gou.github.io/" target="_blank">Mengran Gou</a>*, <a href="http://wuziyan.com/" target="_blank">Ziyan Wu</a>, <a href="https://scholar.google.com/citations?user=YwP6ngkAAAAJ&hl=en&oi=sra" target="_blank">Angels Rates-Borras</a>, <a href="http://www.coe.neu.edu/people/camps-octavia" target="_blank">Octavia Camps</a>, <a href="https://www.ecse.rpi.edu/~rjradke" target="_blank">Richard J. Radke</a>
                  <br>
                  <em>IEEE Transactions on Pattern Analysis and Machine Intelligence (T-PAMI), accepted, Feb 2018.</em> <br>
                  * equal contribution <br>				  
				  <a href="https://github.com/RSL-NEU/person-reid-benchmark" target="_blank">code</a> / <a href="http://robustsystems.coe.neu.edu/sites/robustsystems.coe.neu.edu/files/systems/supmat/ReID_benchmark_supp.zip" target="_blank">supplemental material</a>
				  <p></p>
				  <p> We are conducting a systematic study of existing features, metric learning, and multi-shot ranking algorithms for re-id. Please also see this <a href="http://robustsystems.coe.neu.edu/sites/robustsystems.coe.neu.edu/files/systems/projectpages/reiddataset.html" target="_blank"> collection and review of re-id datasets</a> by <a href="https://neu-gou.github.io/" target="_blank">Mengran.
				  <p></p>
				  </a></p>
              </td>
            </tr>
		  
	  <tr>
              <td width="25%"><img src="papers/2017/figure2.png" alt="conceptGAN" width="180" height="180" style="border-style: none">
              <td width="75%" valign="top">
                <p>
                  <a href="https://arxiv.org/pdf/1711.06148" target="_blank">
                  <papertitle> Learning Compositional Visual Concepts with Mutual Consistency</papertitle>
                  </a>
                  <br>
                  <a href="http://www.bme.cornell.edu/research/groups/doerschuk/people/gong.cfm" target="_blank">Yunye Gong</a>, <strong>Srikrishna Karanam</strong>, <a href="http://wuziyan.com/" target="_blank">Ziyan Wu</a>, <a href="https://scholar.google.com/citations?user=GBi3LYkAAAAJ&hl=en" target="_blank">Kuan-Chuan Peng</a>, <a href="https://scholar.google.com/citations?user=hFUJkFgAAAAJ&hl=en" target="_blank">Jan Ernst</a>, <a href="https://www.bme.cornell.edu/people/profile.cfm?netid=pd83" target="_blank">Peter C. Doerschuk</a>
                  <br>
                  <em>IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</em>, 2018 &nbsp <font color="red"><strong>(Spotlight)</strong></font> <br>			  				  
				  <p></p>
				  <p> We propose ConceptGAN, a framework that can jointly learn, transfer and compose concepts to generate semantically meaningful images, even in subdomains with no training data.
				  <p></p>
				  </a></p>
              </td>
            </tr>

<tr>
              <td width="25%"><img src="papers/2017/keypoints.png" alt="keypoints" width="180" height="180" style="border-style: none">
              <td width="75%" valign="top">
                <p>
                  <a href="https://arxiv.org/pdf/1802.07869" target="_blank">
                  <papertitle> End-to-end learning of keypoint detector and descriptor for pose invariant 3D matching</papertitle>
                  </a>
                  <br>
                  <a href="https://cs.gmu.edu/~ggeorgak/" target="_blank">Georgios Georgakis</a>, <strong>Srikrishna Karanam</strong>, <a href="http://wuziyan.com/" target="_blank">Ziyan Wu</a>, <a href="https://scholar.google.com/citations?user=hFUJkFgAAAAJ&hl=en" target="_blank">Jan Ernst</a>, <a href="https://cs.gmu.edu/~kosecka/" target="_blank">Jana Kosecka</a>
                  <br>
                  <em>IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</em>, 2018 <br>			  				  
				  <p></p>
				  <p> We propose an end-to-end deep network for jointly learning keypoint detection and description from only synthetic data in 3D. The model is a Siamese architecture that integrates Faster R-CNN to generate proposals for validation by a contrastive loss.
				  <p></p>
				  </a></p>
              </td>
            </tr>

              <td width="25%"><img src="papers/2017/rpifield.PNG" alt="rpifield" width="180" height="180" style="border-style: none">
              <td width="75%" valign="top">
                <p>
                  <a href="http://openaccess.thecvf.com/content_cvpr_2018_workshops/papers/w36/Zheng_RPIfield_A_New_CVPR_2018_paper.pdf" target="_blank">
                  <papertitle> RPIfield: A New Dataset for Temporally Evaluating Person Re-Identification</papertitle>
                  </a>
                  <br>
                  <a href="http://homepages.rpi.edu/~zhengm3/" target="_blank">Meng Zheng</a>, <strong>Srikrishna Karanam</strong>, <a href="https://www.ecse.rpi.edu/~rjradke/index.htm" target="_blank">Richard J. Radke</a>
                  <br>
                  <em>IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)</em>, 2018 <br>	
				  <a href="https://github.com/MZhengRPI/RPIfield" target="_blank">data</a>
				  <p></p>
				  <p> We introduce a new multi-shot re-id dataset, called RPIfield, to help evaluate re-id algorithms based on their temporal performance on a dynamic gallery populated by an increasing number of candidates. Data is available <a href="https://github.com/MZhengRPI/RPIfield" target="_blank"> here.</a>
				  <p></p>
				  </a></p>
              </td>
            </tr>
		 			
			<tr>
              <td width="25%"><img src="papers/2017/ranktime.PNG" alt="karanam-arxiv17" width="160" height="120" style="border-style: none">
              <td width="75%" valign="top">
                <p>
                  <a href="https://arxiv.org/pdf/1706.00553.pdf" target="_blank">
                  <papertitle>Rank Persistence: Assessing the Temporal Performance of Real-World Person Re-Identification</papertitle>
                  </a>
                  <br>
                  <strong>Srikrishna Karanam</strong>, Eric Lam, <a href="https://www.ecse.rpi.edu/~rjradke" target="_blank">Richard J. Radke</a>
                  <br>
                  <em>International Conference on Distributed Smart Cameras (ICDSC)</em>, Sept. 2017 <br>				  
				  <p></p>
				  <p> We present a new evaluation methodology that explicitly considers practical aspects involved when deploying a re-id algorithm in the real world. 
				  <p></p>
				  </a></p>
              </td>
            </tr>

 
		  <tr>
              <td width="25%"><img src="papers/2017/csvt17.PNG" alt="karanam-csvt17" width="160" height="180" style="border-style: none">
              <td width="75%" valign="top">
                <p>
                  <a href="https://www.ecse.rpi.edu/~rjradke/papers/karanam-tcsvt17.pdf" target="_blank">
                  <papertitle> Learning Affine Hull Representations for Multi-Shot Person Re-Identification</papertitle>
                  </a>
                  <br>
                  <strong>Srikrishna Karanam</strong>, <a href="http://wuziyan.com/" target="_blank">Ziyan Wu</a>, <a href="https://www.ecse.rpi.edu/~rjradke" target="_blank">Richard J. Radke</a>
                  <br>
                  <em>IEEE Transactions on Circuits and Systems for Video Technology (T-CSVT), accepted July 2017.</em> <br>                 
				  <p></p>
				  <p> We tackle the multi-shot re-id problem by learning discriminative representations using affine hulls of data and show improvements with existing metric learning algorithms.
				  <p></p>
				  </a></p>
              </td>
            </tr>
			
	<tr>
              <td width="25%"><img src="papers/2017/DukeReID.jpg" alt="gou-cvprw17" width="160" height="180" style="border-style: none">
              <td width="75%" valign="top">
                <p>
                  <a href="http://robustsystems.coe.neu.edu/sites/robustsystems.coe.neu.edu/files/systems/papers/MengranGou_CVPRW17.pdf" target="_blank">
                  <papertitle> DukeMTMC4ReID: A Large-Scale Multi-Camera Person Re-Identification Dataset</papertitle>
                  </a>
                  <br>
                  <a href="https://neu-gou.github.io/" target="_blank">Mengran Gou</a>, <strong>Srikrishna Karanam</strong>, Wenqian Liu, <a href="http://www.coe.neu.edu/people/camps-octavia" target="_blank">Octavia Camps</a>, <a href="https://www.ecse.rpi.edu/~rjradke" target="_blank">Richard J. Radke</a>
                  <br>	
		  <em>IEEE Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)</em>, 2017 <br>
				  <a href="https://github.com/NEU-Gou/DukeReID" target="_blank">data</a>
				  <p></p>
				  <p> We introduce a new, large-scale, dataset for re-id based on the <a href="http://vision.cs.duke.edu/DukeMTMC/" target="_blank">DukeMTMC multi-target tracking dataset.
				  <p></p>
				  </a></p>
              </td>
            </tr>
		  
		  <tr>
              <td width="25%"><img src="papers/2015/karanam-msf15-rep.png" alt="karanam-ivc16" width="160" height="120" style="border-style: none">
              <td width="75%" valign="top">
                <p>
                  <a href="https://www.ecse.rpi.edu/~rjradke/papers/karanam-ivc16.pdf" target="_blank">
                  <papertitle>Person Re-Identification with Block Sparse Recovery</papertitle>
                  </a>
                  <br>
                  <strong>Srikrishna Karanam</strong>, Yang Li, <a href="https://www.ecse.rpi.edu/~rjradke" target="_blank">Richard J. Radke</a>
                  <br>
                  <em>Elsevier Image and Vision Computing (IVC)</em>, accepted, Feb. 2017 <br>				  
				  <p></p>
				  <p> We formulate multi-shot re-identification as a block sparse recovery problem. This subsumes our CVPR-W 2015 paper. 
				  <p></p>
				  </a></p>
              </td>
            </tr>
		  
		  
		  
		  <tr>
              <td width="25%"><img src="papers/2016/karanam-csvt16-rep.PNG" alt="karanam-csvt16" width="160" height="120" style="border-style: none">
              <td width="75%" valign="top">
                <p>
                  <a href="https://www.ecse.rpi.edu/~rjradke/papers/radke-csvt16.pdf" target="_blank">
                  <papertitle>From the Lab to the Real World: Re-Identification in an Airport Camera Network</papertitle>
                  </a>
                  <br>
                  <a href="http://www.coe.neu.edu/people/camps-octavia" target="_blank">Octavia Camps</a>, <a href="https://neu-gou.github.io/" target="_blank">Mengran Gou</a>, Tom Hebble, <strong>Srikrishna Karanam</strong>, Oliver Lehmann, Yang Li, <a href="https://www.ecse.rpi.edu/~rjradke" target="_blank">Richard J. Radke</a>, <a href="http://wuziyan.com/" target="_blank">Ziyan Wu</a>, Fei Xiong 
                  <br>
                  <em>IEEE Transactions on Circuits and Systems for Video Technology (T-CSVT)</em>, accepted, Dec. 2016 <br>				  
				  <p></p>
				  <p> This paper describes our real-time end-to-end person re-identification system. This subsumes our ICDSC 2014 paper. 
				  <p></p>
				  </a></p>
              </td>
            </tr>
		  
		  
		  
			<tr>
              <td width="25%"><img src="papers/2015/karanam-iccv15-rep.png" alt="karanam-iccv15" width="160" height="120" style="border-style: none">
              <td width="75%" valign="top">
                <p>
                  <a href="https://www.ecse.rpi.edu/~rjradke/papers/karanam-iccv15.pdf" target="_blank">
                  <papertitle>Person Re-Identification with Discriminatively Trained Viewpoint Invariant Dictionaries</papertitle>
                  </a>
                  <br>
                  <strong>Srikrishna Karanam</strong>, Yang Li, <a href="https://www.ecse.rpi.edu/~rjradke" target="_blank">Richard J. Radke</a> 
                  <br>
                  <em>IEEE International Conference on Computer Vision (ICCV)</em>, 2015 <br>
				  <a href="papers/2015/1167_video.mp4" target="_blank">spotlight</a> / <a href="code/code_iccv15.zip" target="_blank">code / <a href="code/cmc_iccv15.zip" target="_blank">CMC</a></a>
				  <p></p>
				  <p> We learn a dictionary capable of discriminatively and sparsely encoding gallery and probe features.
				  <p></p>
				  </a></p>
              </td>
            </tr>
			
			
			
			<tr>
              <td width="25%"><img src="papers/2015/karanam-bmvc15-rep.png" alt="karanam-bmvc15" width="160" height="120" style="border-style: none">
              <td width="75%" valign="top">
                <p>
                  <a href="https://www.ecse.rpi.edu/~rjradke/papers/karanam-bmvc15.pdf" target="_blank">
                  <papertitle>Particle Dynamics and Multi-Channel Feature Dictionaries for Robust Visual Tracking</papertitle>
                  </a>
                  <br>
                  <strong>Srikrishna Karanam</strong>, Yang Li, <a href="https://www.ecse.rpi.edu/~rjradke" target="_blank">Richard J. Radke</a> 
                  <br>
                  <em>British Machine Vision Conference (BMVC)</em>, 2015 &nbsp <font color="red"><strong>(Oral Presentation)</strong></font>  <br>
                  <a href="papers/2015/karanam-bmvc15-supp.pdf" target="_blank">supplement</a> / <a href="papers/2015/karanam-bmvc15-videos.wmv" target="_blank">videos</a> / <a href="papers/2015/karanam-bmvc15-talk.pptx" target="_blank">slides</a> / <a href="code/code_bmvc15.zip" target="_blank">code</a>
				  <p></p>
				  <p> We construct multi-channel feature dictionaries as part of the target appearance model and exploit particle dynamical information to improve tracking accuracy.
				  <p></p>
				  </a></p>
              </td>
            </tr>
			
			
			
			<tr>
              <td width="25%"><img src="papers/2015/li-bmvc15-rep.png" alt="li-bmvc15" width="160" height="120" style="border-style: none">
              <td width="75%" valign="top">
                <p>
                  <a href="https://www.ecse.rpi.edu/~rjradke/papers/li-bmvc15.pdf" target="_blank">
                  <papertitle>Multi-Shot Human Re-Identification Using Adaptive Fisher Discriminant Analysis </papertitle>
                  </a>
                  <br>
                  Yang Li, <a href="http://wuziyan.com/" target="_blank">Ziyan Wu</a>, <strong>Srikrishna Karanam</strong>, <a href="https://www.ecse.rpi.edu/~rjradke" target="_blank">Richard J. Radke</a> 
                  <br>
                  <em>British Machine Vision Conference (BMVC)</em>, 2015 <br>
				  <p></p>
				  <p> We combine Fisher discriminant analysis and hierarchical image sequence clustering to adaptively learn a discriminative feature space.
				  <p></p>
				  </a></p>
              </td>
            </tr>
			
			<tr>
              <td width="25%"><img src="papers/2015/karanam-msf15-rep.png" alt="karanam-msf15" width="160" height="120" style="border-style: none">
              <td width="75%" valign="top">
                <p>
                  <a href="https://www.ecse.rpi.edu/~rjradke/papers/karanam-msf15.pdf" target="_blank">
                  <papertitle> Sparse Re-Id: Block Sparsity for Person Re-Identification </papertitle>
                  </a>
                  <br>
                  <strong>Srikrishna Karanam</strong>, Yang Li, <a href="https://www.ecse.rpi.edu/~rjradke" target="_blank">Richard J. Radke</a> 
                  <br>
                  <em>IEEE Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)</em>, 2015 <br>
				  <p></p>
				  <p> We formulate multi-shot re-identification as a block sparse recovery problem.
				  <p></p>
				  </a></p>
              </td>
            </tr>
			
			
			<tr>
              <td width="25%"><img src="papers/2014/li-icdsc14-rep.png" alt="li-icsdc15" width="160" height="120" style="border-style: none">
              <td width="75%" valign="top">
                <p>
                  <a href="https://www.ecse.rpi.edu/~rjradke/papers/radke-icdsc14.pdf" target="_blank">
                  <papertitle>Real-World Re-Identification in an Airport Camera Network </papertitle>
                  </a>
                  <br>
			Yang Li, <a href="http://wuziyan.com/" target="_blank">Ziyan Wu</a>, <strong>Srikrishna Karanam</strong>, <a href="https://www.ecse.rpi.edu/~rjradke" target="_blank">Richard J. Radke</a> 
                  <br>
                  <em>International Conference on Distributed Smart Cameras (ICDSC)</em>, Nov. 2014 <br>
				  <a href="https://www.youtube.com/watch?v=P6vbn0c4JmY&list=PLuh62Q4Sv7BVld6hKiIjCGUZhdqGiamfv&index=4" target="_blank">Demo video</a> 
				  <p></p>
				  <p> This paper describes a preliminary version of our real-time end-to-end person re-identification system.
				  <p></p>
				  </a></p>
              </td>
            </tr>
			

          </table>


		<script>
		  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
		  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
		  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
		  })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

		  ga('create', 'UA-33172405-3', 'auto');
		  ga('send', 'pageview');

		</script>

		  
          <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
            <tr>
              <td>
                <br>
                <p align="right"><font size="2">
                  <a href="http://www.cs.berkeley.edu/~barron/" target="_blank">Template credits</a>
                  </font>
                </p>
              </td>
            </tr>
          </table>

        </td>
      </tr>
    </table>
  </body>
</html>
