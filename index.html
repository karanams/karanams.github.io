<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<html>
  <head>
	<meta name=viewport content='width=800'>
    <meta name="generator" content="HTML Tidy for Linux/x86 (vers 11 February 2007), see www.w3.org">
    <style type="text/css">
      /* Color scheme from Sergey Karayev and Jon Barron*/
      a {
      color: #1772d0;
      text-decoration:none;
      }
      a:focus, a:hover {
      color: #f09228;
      text-decoration:none;
      }
      body,td,th {
      font-family: 'Lato', Verdana, Helvetica, sans-serif;
      font-size: 14px
      }
      strong {
      font-family: 'Lato', Verdana, Helvetica, sans-serif;
      font-size: 14px;
      }
      heading {
      font-family: 'Lato', Verdana, Helvetica, sans-serif;
      font-size: 18px;
      }
      papertitle {
      font-family: 'Lato', Verdana, Helvetica, sans-serif;
      font-size: 15px;
      font-weight: 700
      }
      name {
      font-family: 'Lato', Verdana, Helvetica, sans-serif;
      font-size: 32px;
      }
	.fade {
	   transition: opacity .2s ease-in-out;
	   -moz-transition: opacity .2s ease-in-out;
	   -webkit-transition: opacity .2s ease-in-out;
	   }
    </style>

    <title>Srikrishna Karanam</title>
    <meta http-equiv="Content-Type" content="text/html; charset=us-ascii">
    <link href='http://fonts.googleapis.com/css?family=Lato:400,700,400italic,700italic' rel='stylesheet' type='text/css'>
  </head>
  <body>
    <table width="800" border="0" align="center" cellspacing="0" cellpadding="0">
      <tr>
        <td>
          <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
            <tr>
              <td width="67%" valign="middle">
                <p align="center">
                  <name>Srikrishna Karanam</name>
                  </font>
                <p align>I am part of the Vision team at <a href="https://usa.united-imaging.com/" target="_blank">United Imaging Intelligence</a>, where I work on Computer Vision, Machine Learning, and related problems. I earned my Ph.D. in <a href="https://www.ecse.rpi.edu/" target="_blank">Computer and Systems Engineering</a> at <a href="https://www.rpi.edu/" target="_blank">Rensselaer Polytechnic Institute</a> in 2017, where my adviser was <a href="https://www.ecse.rpi.edu/~rjradke" target="_blank">Prof. Rich Radke</a>.</p>
                <p align=center>
<a href="mailto:srikrishna@ieee.org">Email</a> &nbsp/&nbsp
<a href="https://scholar.google.com/citations?user=G2-skyUAAAAJ&hl=en&oi=ao" target="_blank">Google Scholar</a> &nbsp/&nbsp
<a href="https://www.linkedin.com/in/srikrishna-karanam-06b43127" target="_blank">LinkedIn</a>
                </p>
              </td>
              <td width="33%"><img src="karanam1.jpg" height=300></td>
            </tr>
          </table>
          <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
			

	  <tr><td>
            <heading>Recent events</heading>
            <ul>	    
	    <li> First Workshop on <a href="https://fadetrcv.github.io/" target="_blank">Fair, Data-efficient, and Trusted Computer Vision</a>, <a href="http://cvpr2020.thecvf.com/" target="_blank">CVPR 2020</a>. </li>
            <li> Second Workshop on <a href="https://wvbsd.github.io/2019/" target="_blank">Vision with Biased or Scarce Data</a>, <a href="http://cvpr2019.thecvf.com/" target="_blank">CVPR 2019</a>. </li>  	    
            </ul>
		</td></tr>
		    
		  
            <tr>
              <td>
                <heading>Research</heading>
                <p>Computer Vision and Machine Learning with recent focus on robust and explainable perception.
				
              </td>
            </tr>
          </table>


</tr>
		  
          <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
		  
              <tr>
              <td>
                <heading>Papers</heading>
              </td>
            </tr>
		  
		  </tr>

              <td width="25%"><img src="papers/2020/hkmrTeaser.pdf" alt="teaser" width="180" height="90" style="border-style: none">
              <td width="75%" valign="top">
                <p>
                  <a href="https://arxiv.org/pdf/2003.04232.pdf" target="_blank">
                  <papertitle>Hierarchical Kinematic Human Mesh Recovery</papertitle>
                  </a>
                  <br>
                  <a href="https://cs.gmu.edu/~ggeorgak/" target="_blank">Georgios Georgakis</a>*, <a href="https://liren2515.github.io/page/" target="_blank">Ren Li</a>**, <strong>Srikrishna Karanam</strong>, <a href="https://scholar.google.com/citations?user=S2BT6ogAAAAJ&hl=en" target="_blank">Terrence Chen</a>, <a href="https://cs.gmu.edu/~kosecka/" target="_blank">Jana Kosecka</a>, <a href="http://wuziyan.com/" target="_blank">Ziyan Wu</a>
                  <br>
                  <em>European Conference on Computer Vision (<strong>ECCV</strong>)</em>, 2020  <br>	
                  * equal contribution <br>
				  <p></p>
				  <p> We present a new structure-aware design and an associated hierarchical optimization scheme for fitting a parameteric 3D human model to a single image of a person.

				  <p></p>
				  </a></p>
              </td>
            </tr>
		  
		  
		  </tr>

              <td width="25%"><img src="papers/2020/vaeTeaser.PNG" alt="teaser" width="180" height="90" style="border-style: none">
              <td width="75%" valign="top">
                <p>
                  <a href="https://arxiv.org/pdf/1911.07389.pdf" target="_blank">
                  <papertitle>Towards Visually Explaining Variational Autoencoders</papertitle>
                  </a>
                  <br>
                  <a href="https://scholar.google.com/citations?user=6yh4f4YAAAAJ&hl=en" target="_blank">Wenqian Liu</a>*, <a href="https://scholar.google.com/citations?user=bCvdh54AAAAJ&hl=en" target="_blank">Runze Li</a>*, <a href="https://scholar.google.com/citations?user=1D5PfMgAAAAJ&hl=en" target="_blank">Meng Zheng</a>, <strong>Srikrishna Karanam</strong>, <a href="http://wuziyan.com/" target="_blank">Ziyan Wu</a>, <a href="https://vislab.ucr.edu/PEOPLE/BIR_BHANU/index.php" target="_blank">Bir Bhanu</a>, <a href="https://www.ecse.rpi.edu/~rjradke/index.htm" target="_blank">Richard J. Radke</a>, and <a href="https://coe.northeastern.edu/people/camps-octavia/" target="_blank">Octavia Camps</a>
                  <br>
                  <em>IEEE/CVF Conference on Computer Vision and Pattern Recognition (<strong>CVPR</strong>)</em>, 2020 &nbsp <font color="red"><strong>(Oral)</strong></font> <br>	
                  * equal contribution <br>
				  <p></p>
				  <p> We take a step towards visually explaining deep generative models by presenting the first technique to generate visual attention from the learned latent space of a VAE. We show how the resulting visual attention can be applied to problems such as anomaly localization and disentangled representation learning.  

				  <p></p>
				  </a></p>
              </td>
            </tr>




		  
		  </tr>

              <td width="25%"><img src="papers/2018/pipeline.PNG" alt="teaser" width="180" height="90" style="border-style: none">
              <td width="75%" valign="top">
                <p>
                  <a href="https://arxiv.org/pdf/1811.12297.pdf" target="_blank">
                  <papertitle>Incremental Scene Synthesis</papertitle>
                  </a>
                  <br>
                  <a href="https://planche.me/" target="_blank">Benjamin Planche</a>, <a href="http://xrong.org/" target="_blank">Xuejian Rong</a>, <a href="http://wuziyan.com/" target="_blank">Ziyan Wu</a>, <strong>Srikrishna Karanam</strong>, <a href="http://www.fim.uni-passau.de/en/distributed-information-systems/" target="_blank">Harald Kosch</a>, <a href="http://media-lab.ccny.cuny.edu/wordpress/YLTCCNYHomepage/home.html" target="_blank">YingLi Tian</a>, <a href="https://scholar.google.de/citations?user=hFUJkFgAAAAJ&hl=en" target="_blank">Jan Ernst</a>, <a href="https://www.researchgate.net/profile/Andreas_Hutter" target="_blank">Andreas Hutter</a>
                  <br>
                  <em>Annual Conference on Neural Information Processing Systems (<strong>NeurIPS</strong>)</em>, 2019 <br>	
				  <p></p>
				  <p> We present a method to incrementally generate complete 2D or 3D scenes. Our framework can register observations from a non-localized agent in a global representation, which can be used to synthesize new views as well as fill in gaps in the representation while observing global consistency.  

				  <p></p>
				  </a></p>
              </td>
            </tr>
		  

<tr>
              <td width="25%"><img src="papers/2018/title.PNG" alt="rgb2cad" width="180" height="180" style="border-style: none">
              <td width="75%" valign="top">
                <p>
                  <a href="https://arxiv.org/pdf/1811.07249" target="_blank">
                  <papertitle>Learning Local RGB-to-CAD Correspondences for Object Pose Estimation
</papertitle>
                  </a>
                  <br>
                  <a href="https://cs.gmu.edu/~ggeorgak/" target="_blank">Georgios Georgakis</a>, <strong>Srikrishna Karanam</strong>, <a href="http://wuziyan.com/" target="_blank">Ziyan Wu</a>, <a href="https://cs.gmu.edu/~kosecka/" target="_blank">Jana Kosecka</a>
                  <br>
                  <em>IEEE/CVF International Conference on Computer Vision (<strong>ICCV</strong>)</em>, 2019 <br>
				  <p></p>
				  <p> We present a new method that matches RGB images to rendered depth images of CAD models for object pose estimation. The method does not require either textured CAD models or 3D pose annotations for RGB images during training. This is achieved through a series of constraints that enforce viewpoint and modality invariance for local features, and learn how to select keypoints consistently across modalities.
				  <p></p>
				  </a></p>
              </td>
		  
 	   
		              </tr>
		  
		  </tr>

              <td width="25%"><img src="papers/2018/mainidea.png" alt="discAtten" width="180" height="90" style="border-style: none">
              <td width="75%" valign="top">
                <p>
                  <a href="https://arxiv.org/pdf/1811.07484" target="_blank">
                  <papertitle>Sharpen Focus: Learning with Attention Separability and Consistency</papertitle>
                  </a>
                  <br>
                  <a href="https://scholar.google.com/citations?user=B6JR50gAAAAJ&hl=en" target="_blank">Lezi Wang</a>, <a href="http://wuziyan.com/" target="_blank">Ziyan Wu</a>, <strong>Srikrishna Karanam</strong>, <a href="https://scholar.google.com/citations?user=GBi3LYkAAAAJ&hl=en" target="_blank">Kuan-Chuan Peng, <a href="http://rajatvikramsingh.github.io/" target="_blank">Rajat Vikram Singh</a>, <a href="https://scholar.google.com/citations?user=2Fe93n8AAAAJ&hl=en" target="_blank">Bo Liu</a>, <a href="https://www.cs.rutgers.edu/~dnm/" target="_blank">Dimitris N. Metaxas</a>
                  <br>
                  <em>IEEE/CVF International Conference on Computer Vision (<strong>ICCV</strong>)</em>, 2019 <br>	
				  <p></p>
				  <p> We present a new learning framework that makes discriminative attention a principled part of the learning process, showing it can lead to better model interpretability, as well as improved classification accuracy.
				  <p></p>
				  </a></p>
              </td>
            </tr>
		  
</tr>

              <td width="25%"><img src="papers/2018/teaser.png" alt="teaser" width="180" height="120" style="border-style: none">
              <td width="75%" valign="top">
                <p>
                  <a href="https://arxiv.org/pdf/1811.07487" target="_blank">
                  <papertitle>Re-Identification with Consistent Attentive Siamese Networks</papertitle>
                  </a>
                  <br>
                  <a href="http://homepages.rpi.edu/~zhengm3/" target="_blank">Meng Zheng</a>, <strong>Srikrishna Karanam</strong>, <a href="http://wuziyan.com/" target="_blank">Ziyan Wu</a>, <a href="https://www.ecse.rpi.edu/~rjradke/index.htm" target="_blank">Richard J. Radke</a>
                  <br>
                  <em>IEEE/CVF Conference on Computer Vision and Pattern Recognition (<strong>CVPR</strong>)</em>, 2019 <br>	
				  <p></p>
				  <p> We present the first framework for re-id that provides mechanisms to make attention and attention consistency end-to-end trainable in a Siamese learning architecture, resulting in a technique for robust cross-view matching as well as explaining the reasoning for why the model predicts that the two images belong to the same person.
				  <p></p>
				  </a></p>
              </td>
            </tr>
		  
		  <tr>
              <td width="25%"><img src="papers/2016/teaser-pami.PNG" alt="karanam-gou-arxiv16" width="160" height="180" style="border-style: none">
              <td width="75%" valign="top">
                <p>
                  <a href="https://arxiv.org/pdf/1605.09653.pdf" target="_blank">
                  <papertitle> A Systematic Evaluation and Benchmark for Person Re-Identification: Features, Metrics, and Datasets</papertitle>
                  </a>
                  <br>
                  <strong>Srikrishna Karanam</strong>*, <a href="https://neu-gou.github.io/" target="_blank">Mengran Gou</a>*, <a href="http://wuziyan.com/" target="_blank">Ziyan Wu</a>, <a href="https://scholar.google.com/citations?user=YwP6ngkAAAAJ&hl=en&oi=sra" target="_blank">Angels Rates-Borras</a>, <a href="http://www.coe.neu.edu/people/camps-octavia" target="_blank">Octavia Camps</a>, <a href="https://www.ecse.rpi.edu/~rjradke" target="_blank">Richard J. Radke</a>
                  <br>
                  <em>IEEE Transactions on Pattern Analysis and Machine Intelligence (<strong>T-PAMI</strong>), accepted, Feb 2018.</em> <br>
                  * equal contribution <br>				  
				  <a href="https://github.com/RSL-NEU/person-reid-benchmark" target="_blank">code</a> / <a href="http://robustsystems.coe.neu.edu/sites/robustsystems.coe.neu.edu/files/systems/supmat/ReID_benchmark_supp.zip" target="_blank">supplemental material</a>
				  <p></p>
				  <p> We are conducting a systematic study of existing features, metric learning, and multi-shot ranking algorithms for re-id. Please also see this <a href="http://robustsystems.coe.neu.edu/sites/robustsystems.coe.neu.edu/files/systems/projectpages/reiddataset.html" target="_blank"> collection and review of re-id datasets</a> by <a href="https://neu-gou.github.io/" target="_blank">Mengran.
				  <p></p>
				  </a></p>
              </td>
            </tr>
		  
	  <tr>
              <td width="25%"><img src="papers/2017/figure2.png" alt="conceptGAN" width="180" height="180" style="border-style: none">
              <td width="75%" valign="top">
                <p>
                  <a href="https://arxiv.org/pdf/1711.06148" target="_blank">
                  <papertitle> Learning Compositional Visual Concepts with Mutual Consistency</papertitle>
                  </a>
                  <br>
                  <a href="http://www.bme.cornell.edu/research/groups/doerschuk/people/gong.cfm" target="_blank">Yunye Gong</a>, <strong>Srikrishna Karanam</strong>, <a href="http://wuziyan.com/" target="_blank">Ziyan Wu</a>, <a href="https://scholar.google.com/citations?user=GBi3LYkAAAAJ&hl=en" target="_blank">Kuan-Chuan Peng</a>, <a href="https://scholar.google.com/citations?user=hFUJkFgAAAAJ&hl=en" target="_blank">Jan Ernst</a>, <a href="https://www.bme.cornell.edu/people/profile.cfm?netid=pd83" target="_blank">Peter C. Doerschuk</a>
                  <br>
                  <em>IEEE/CVF Conference on Computer Vision and Pattern Recognition (<strong>CVPR</strong>)</em>, 2018 &nbsp <font color="red"><strong>(Spotlight)</strong></font> <br>			  				  
				  <p></p>
				  <p> We propose ConceptGAN, a framework that can jointly learn, transfer and compose concepts to generate semantically meaningful images, even in subdomains with no training data.
				  <p></p>
				  </a></p>
              </td>
            </tr>

<tr>
              <td width="25%"><img src="papers/2017/keypoints.png" alt="keypoints" width="180" height="180" style="border-style: none">
              <td width="75%" valign="top">
                <p>
                  <a href="https://arxiv.org/pdf/1802.07869" target="_blank">
                  <papertitle> End-to-end learning of keypoint detector and descriptor for pose invariant 3D matching</papertitle>
                  </a>
                  <br>
                  <a href="https://cs.gmu.edu/~ggeorgak/" target="_blank">Georgios Georgakis</a>, <strong>Srikrishna Karanam</strong>, <a href="http://wuziyan.com/" target="_blank">Ziyan Wu</a>, <a href="https://scholar.google.com/citations?user=hFUJkFgAAAAJ&hl=en" target="_blank">Jan Ernst</a>, <a href="https://cs.gmu.edu/~kosecka/" target="_blank">Jana Kosecka</a>
                  <br>
                  <em>IEEE/CVF Conference on Computer Vision and Pattern Recognition (<strong>CVPR</strong>)</em>, 2018 <br>			  				  
				  <p></p>
				  <p> We propose an end-to-end deep network for jointly learning keypoint detection and description from only synthetic data in 3D. The model is a Siamese architecture that integrates Faster R-CNN to generate proposals for validation by a contrastive loss.
				  <p></p>
				  </a></p>
              </td>
            </tr>

              <td width="25%"><img src="papers/2017/rpifield.PNG" alt="rpifield" width="180" height="180" style="border-style: none">
              <td width="75%" valign="top">
                <p>
                  <a href="http://openaccess.thecvf.com/content_cvpr_2018_workshops/papers/w36/Zheng_RPIfield_A_New_CVPR_2018_paper.pdf" target="_blank">
                  <papertitle> RPIfield: A New Dataset for Temporally Evaluating Person Re-Identification</papertitle>
                  </a>
                  <br>
                  <a href="http://homepages.rpi.edu/~zhengm3/" target="_blank">Meng Zheng</a>, <strong>Srikrishna Karanam</strong>, <a href="https://www.ecse.rpi.edu/~rjradke/index.htm" target="_blank">Richard J. Radke</a>
                  <br>
                  <em>IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (<strong>CVPRW</strong>)</em>, 2018 <br>	
				  <a href="https://github.com/MZhengRPI/RPIfield" target="_blank">data</a>
				  <p></p>
				  <p> We introduce a new multi-shot re-id dataset, called RPIfield, to help evaluate re-id algorithms based on their temporal performance on a dynamic gallery populated by an increasing number of candidates. Data is available <a href="https://github.com/MZhengRPI/RPIfield" target="_blank"> here.</a>
				  <p></p>
				  </a></p>
              </td>
            </tr>
		 			
			<tr>
              <td width="25%"><img src="papers/2017/ranktime.PNG" alt="karanam-arxiv17" width="160" height="120" style="border-style: none">
              <td width="75%" valign="top">
                <p>
                  <a href="https://arxiv.org/pdf/1706.00553.pdf" target="_blank">
                  <papertitle>Rank Persistence: Assessing the Temporal Performance of Real-World Person Re-Identification</papertitle>
                  </a>
                  <br>
                  <strong>Srikrishna Karanam</strong>, Eric Lam, <a href="https://www.ecse.rpi.edu/~rjradke" target="_blank">Richard J. Radke</a>
                  <br>
                  <em>International Conference on Distributed Smart Cameras (<strong>ICDSC</strong>)</em>, Sept. 2017 <br>				  
				  <p></p>
				  <p> We present a new evaluation methodology that explicitly considers practical aspects involved when deploying a re-id algorithm in the real world. 
				  <p></p>
				  </a></p>
              </td>
            </tr>

 
		  <tr>
              <td width="25%"><img src="papers/2017/csvt17.PNG" alt="karanam-csvt17" width="160" height="180" style="border-style: none">
              <td width="75%" valign="top">
                <p>
                  <a href="https://www.ecse.rpi.edu/~rjradke/papers/karanam-tcsvt17.pdf" target="_blank">
                  <papertitle> Learning Affine Hull Representations for Multi-Shot Person Re-Identification</papertitle>
                  </a>
                  <br>
                  <strong>Srikrishna Karanam</strong>, <a href="http://wuziyan.com/" target="_blank">Ziyan Wu</a>, <a href="https://www.ecse.rpi.edu/~rjradke" target="_blank">Richard J. Radke</a>
                  <br>
                  <em>IEEE Transactions on Circuits and Systems for Video Technology (<strong>T-CSVT</strong>), accepted July 2017.</em> <br>                 
				  <p></p>
				  <p> We tackle the multi-shot re-id problem by learning discriminative representations using affine hulls of data and show improvements with existing metric learning algorithms.
				  <p></p>
				  </a></p>
              </td>
            </tr>
			
	<tr>
              <td width="25%"><img src="papers/2017/DukeReID.jpg" alt="gou-cvprw17" width="160" height="180" style="border-style: none">
              <td width="75%" valign="top">
                <p>
                  <a href="http://robustsystems.coe.neu.edu/sites/robustsystems.coe.neu.edu/files/systems/papers/MengranGou_CVPRW17.pdf" target="_blank">
                  <papertitle> DukeMTMC4ReID: A Large-Scale Multi-Camera Person Re-Identification Dataset</papertitle>
                  </a>
                  <br>
                  <a href="https://neu-gou.github.io/" target="_blank">Mengran Gou</a>, <strong>Srikrishna Karanam</strong>, Wenqian Liu, <a href="http://www.coe.neu.edu/people/camps-octavia" target="_blank">Octavia Camps</a>, <a href="https://www.ecse.rpi.edu/~rjradke" target="_blank">Richard J. Radke</a>
                  <br>	
		  <em>IEEE Conference on Computer Vision and Pattern Recognition Workshops (<strong>CVPRW</strong>)</em>, 2017 <br>
				  <a href="https://github.com/NEU-Gou/DukeReID" target="_blank">data</a>
				  <p></p>
				  <p> We introduce a new, large-scale, dataset for re-id based on the <a href="http://vision.cs.duke.edu/DukeMTMC/" target="_blank">DukeMTMC multi-target tracking dataset.
				  <p></p>
				  </a></p>
              </td>
            </tr>
		  
		  <tr>
              <td width="25%"><img src="papers/2015/karanam-msf15-rep.png" alt="karanam-ivc16" width="160" height="120" style="border-style: none">
              <td width="75%" valign="top">
                <p>
                  <a href="https://www.ecse.rpi.edu/~rjradke/papers/karanam-ivc16.pdf" target="_blank">
                  <papertitle>Person Re-Identification with Block Sparse Recovery</papertitle>
                  </a>
                  <br>
                  <strong>Srikrishna Karanam</strong>, Yang Li, <a href="https://www.ecse.rpi.edu/~rjradke" target="_blank">Richard J. Radke</a>
                  <br>
                  <em>Elsevier Image and Vision Computing (<strong>IVC</strong>)</em>, accepted, Feb. 2017 <br>				  
				  <p></p>
				  <p> We formulate multi-shot re-identification as a block sparse recovery problem. This subsumes our CVPR-W 2015 paper. 
				  <p></p>
				  </a></p>
              </td>
            </tr>
		  
		  
		  
		  <tr>
              <td width="25%"><img src="papers/2016/karanam-csvt16-rep.PNG" alt="karanam-csvt16" width="160" height="120" style="border-style: none">
              <td width="75%" valign="top">
                <p>
                  <a href="https://www.ecse.rpi.edu/~rjradke/papers/radke-csvt16.pdf" target="_blank">
                  <papertitle>From the Lab to the Real World: Re-Identification in an Airport Camera Network</papertitle>
                  </a>
                  <br>
                  <a href="http://www.coe.neu.edu/people/camps-octavia" target="_blank">Octavia Camps</a>, <a href="https://neu-gou.github.io/" target="_blank">Mengran Gou</a>, Tom Hebble, <strong>Srikrishna Karanam</strong>, Oliver Lehmann, Yang Li, <a href="https://www.ecse.rpi.edu/~rjradke" target="_blank">Richard J. Radke</a>, <a href="http://wuziyan.com/" target="_blank">Ziyan Wu</a>, Fei Xiong 
                  <br>
                  <em>IEEE Transactions on Circuits and Systems for Video Technology (<strong>T-CSVT</strong>)</em>, accepted, Dec. 2016 <br>				  
				  <p></p>
				  <p> This paper describes our real-time end-to-end person re-identification system. This subsumes our ICDSC 2014 paper. 
				  <p></p>
				  </a></p>
              </td>
            </tr>
		  
		  
		  
			<tr>
              <td width="25%"><img src="papers/2015/karanam-iccv15-rep.png" alt="karanam-iccv15" width="160" height="120" style="border-style: none">
              <td width="75%" valign="top">
                <p>
                  <a href="https://www.ecse.rpi.edu/~rjradke/papers/karanam-iccv15.pdf" target="_blank">
                  <papertitle>Person Re-Identification with Discriminatively Trained Viewpoint Invariant Dictionaries</papertitle>
                  </a>
                  <br>
                  <strong>Srikrishna Karanam</strong>, Yang Li, <a href="https://www.ecse.rpi.edu/~rjradke" target="_blank">Richard J. Radke</a> 
                  <br>
                  <em>IEEE International Conference on Computer Vision (<strong>ICCV</strong>)</em>, 2015 <br>
				  <a href="papers/2015/1167_video.mp4" target="_blank">spotlight</a> / <a href="code/code_iccv15.zip" target="_blank">code / <a href="code/cmc_iccv15.zip" target="_blank">CMC</a></a>
				  <p></p>
				  <p> We learn a dictionary capable of discriminatively and sparsely encoding gallery and probe features.
				  <p></p>
				  </a></p>
              </td>
            </tr>
			
			
			
			<tr>
              <td width="25%"><img src="papers/2015/karanam-bmvc15-rep.png" alt="karanam-bmvc15" width="160" height="120" style="border-style: none">
              <td width="75%" valign="top">
                <p>
                  <a href="https://www.ecse.rpi.edu/~rjradke/papers/karanam-bmvc15.pdf" target="_blank">
                  <papertitle>Particle Dynamics and Multi-Channel Feature Dictionaries for Robust Visual Tracking</papertitle>
                  </a>
                  <br>
                  <strong>Srikrishna Karanam</strong>, Yang Li, <a href="https://www.ecse.rpi.edu/~rjradke" target="_blank">Richard J. Radke</a> 
                  <br>
                  <em>British Machine Vision Conference (<strong>BMVC</strong>)</em>, 2015 &nbsp <font color="red"><strong>(Oral)</strong></font>  <br>
                  <a href="papers/2015/karanam-bmvc15-supp.pdf" target="_blank">supplement</a> / <a href="papers/2015/karanam-bmvc15-videos.wmv" target="_blank">videos</a> / <a href="papers/2015/karanam-bmvc15-talk.pptx" target="_blank">slides</a> / <a href="code/code_bmvc15.zip" target="_blank">code</a>
				  <p></p>
				  <p> We construct multi-channel feature dictionaries as part of the target appearance model and exploit particle dynamical information to improve tracking accuracy.
				  <p></p>
				  </a></p>
              </td>
            </tr>
			
			
			
			<tr>
              <td width="25%"><img src="papers/2015/li-bmvc15-rep.png" alt="li-bmvc15" width="160" height="120" style="border-style: none">
              <td width="75%" valign="top">
                <p>
                  <a href="https://www.ecse.rpi.edu/~rjradke/papers/li-bmvc15.pdf" target="_blank">
                  <papertitle>Multi-Shot Human Re-Identification Using Adaptive Fisher Discriminant Analysis </papertitle>
                  </a>
                  <br>
                  Yang Li, <a href="http://wuziyan.com/" target="_blank">Ziyan Wu</a>, <strong>Srikrishna Karanam</strong>, <a href="https://www.ecse.rpi.edu/~rjradke" target="_blank">Richard J. Radke</a> 
                  <br>
                  <em>British Machine Vision Conference (<strong>BMVC</strong>)</em>, 2015 <br>
				  <p></p>
				  <p> We combine Fisher discriminant analysis and hierarchical image sequence clustering to adaptively learn a discriminative feature space.
				  <p></p>
				  </a></p>
              </td>
            </tr>
			
			<tr>
              <td width="25%"><img src="papers/2015/karanam-msf15-rep.png" alt="karanam-msf15" width="160" height="120" style="border-style: none">
              <td width="75%" valign="top">
                <p>
                  <a href="https://www.ecse.rpi.edu/~rjradke/papers/karanam-msf15.pdf" target="_blank">
                  <papertitle> Sparse Re-Id: Block Sparsity for Person Re-Identification </papertitle>
                  </a>
                  <br>
                  <strong>Srikrishna Karanam</strong>, Yang Li, <a href="https://www.ecse.rpi.edu/~rjradke" target="_blank">Richard J. Radke</a> 
                  <br>
                  <em>IEEE Conference on Computer Vision and Pattern Recognition Workshops (<strong>CVPRW</strong>)</em>, 2015 <br>
				  <p></p>
				  <p> We formulate multi-shot re-identification as a block sparse recovery problem.
				  <p></p>
				  </a></p>
              </td>
            </tr>
			
			
			<tr>
              <td width="25%"><img src="papers/2014/li-icdsc14-rep.png" alt="li-icsdc15" width="160" height="120" style="border-style: none">
              <td width="75%" valign="top">
                <p>
                  <a href="https://www.ecse.rpi.edu/~rjradke/papers/radke-icdsc14.pdf" target="_blank">
                  <papertitle>Real-World Re-Identification in an Airport Camera Network </papertitle>
                  </a>
                  <br>
			Yang Li, <a href="http://wuziyan.com/" target="_blank">Ziyan Wu</a>, <strong>Srikrishna Karanam</strong>, <a href="https://www.ecse.rpi.edu/~rjradke" target="_blank">Richard J. Radke</a> 
                  <br>
                  <em>International Conference on Distributed Smart Cameras (<strong>ICDSC</strong>)</em>, Nov. 2014 <br>
				  <a href="https://www.youtube.com/watch?v=P6vbn0c4JmY&list=PLuh62Q4Sv7BVld6hKiIjCGUZhdqGiamfv&index=4" target="_blank">Demo video</a> 
				  <p></p>
				  <p> This paper describes a preliminary version of our real-time end-to-end person re-identification system.
				  <p></p>
				  </a></p>
              </td>
            </tr>
			

          </table>


		<script>
		  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
		  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
		  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
		  })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

		  ga('create', 'UA-33172405-3', 'auto');
		  ga('send', 'pageview');

		</script>

		  
          <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
            <tr>
              <td>
                <br>
                <p align="right"><font size="2">
                  <a href="http://www.cs.berkeley.edu/~barron/" target="_blank">Template credits</a>
                  </font>
                </p>
              </td>
            </tr>
          </table>

        </td>
      </tr>
    </table>
  </body>
</html>
